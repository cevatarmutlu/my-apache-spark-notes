{
 "cells": [
  {
   "source": [
    "findspark: Spark' ı Python Script' lerinde kullanbilmeyi sağlayan yapı.\n",
    "\n",
    "SparkContext is the entry point to any spark functionality. When we run any Spark application, a driver program starts, which has the main function and your SparkContext gets initiated here. The driver program then runs the operations inside the executors on worker nodes. \n",
    "\n",
    "SparkContext uses Py4J to launch a JVM and creates a JavaSparkContext [[Tutorials Points](https://www.tutorialspoint.com/pyspark/pyspark_sparkcontext.htm)]"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "import findspark\n",
    "findspark.init()"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(master=\"local\", appName=\"firtsapp\")\n",
    "# parametre\n",
    "# master: It is the URL of the cluster it connects to. (anlamadım)\n",
    "\n",
    "# Bu hücreyi bir kez daha çalıştırsanız şöyle bir hata alıcaksınız:\n",
    "# ValueError: Cannot run multiple SparkContexts at once"
   ]
  },
  {
   "source": [
    "Dizindeki README.md dosyasının içinde `a` ve `b` karakteri geçen satırların sayısını buluyoruz."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Lines with a: 53, lines with b: 26\n"
     ]
    }
   ],
   "source": [
    "file = sc.textFile(\"README.md\").cache()\n",
    "rowA = file.filter(lambda row: 'a' in row).count()\n",
    "rowB =file.filter(lambda row: 'b' in row).count()\n",
    "print(f\"Lines with a: {rowA}, lines with b: {rowB}\")"
   ]
  },
  {
   "source": [
    "##  PySpark RDD\n",
    "\n",
    "RDD stands for Resilient Distributed Dataset, these are the elements that run and operate on multiple nodes to do parallel processing on a cluster.\n",
    "\n",
    "RDDs are immutable elements, which means once you create an RDD you cannot change it. RDDs are fault tolerant as well, hence in case of any failure, they recover automatically.\n",
    "\n",
    "To apply operations on these RDD's, there are two ways −\n",
    "- Transformation and\n",
    "- Action\n",
    "\n",
    "Transformation − These are the operations, which are applied on a RDD to create a new RDD. Filter, groupBy and map are the examples of transformations.\n",
    "\n",
    "Action − These are the operations that are applied on RDD, which instructs Spark to perform computation and send the result back to the driver.\n",
    "\n",
    "To apply any operation in PySpark, we need to create a PySpark RDD first. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = [\n",
    "    \"scala\", \n",
    "    \"java\", \n",
    "    \"hadoop\", \n",
    "    \"spark\", \n",
    "    \"akka\",\n",
    "    \"spark vs hadoop\", \n",
    "    \"pyspark\",\n",
    "    \"pyspark and spark\"\n",
    "]\n",
    "words = sc.parallelize(arr) # RDD nesnesi oluşturur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "8\ntype: <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "# count(): RDD' de bulunan element' lerin sayısını döndürür.\n",
    "words.count()\n",
    "print(count)\n",
    "print(\"type:\", type(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['scala', 'java', 'hadoop', 'spark', 'akka', 'spark vs hadoop', 'pyspark', 'pyspark and spark']\ntype:  <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# collect(): RDD' deki bütün element' leri döndürür\n",
    "collect = words.collect() # RDD to Array\n",
    "print(collect)\n",
    "print(\"type: \", type(collect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# foreach(): foreach fonksiyonuna parametre olarak verilen fonksiyonu RDD içindeki bütün elemanlara uygular\n",
    "def f(x):\n",
    "    print(\"Merhaba dünya\", x)\n",
    "words.foreach(f) # Çalışmaıyor. Neden bilmiyorum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['spark', 'spark vs hadoop', 'pyspark', 'pyspark and spark']"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "# filter(): Şartı sağlayan elemanlardan yeni bir RDD oluşturur\n",
    "filtered = words.filter(lambda x: 'spark' in x)\n",
    "filtered.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('scala', 1),\n",
       " ('java', 1),\n",
       " ('hadoop', 1),\n",
       " ('spark', 1),\n",
       " ('akka', 1),\n",
       " ('spark vs hadoop', 1),\n",
       " ('pyspark', 1),\n",
       " ('pyspark and spark', 1)]"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "# map(): RDD' deki her bir element' e aynı işlemi uygulayarak yeni bir RDD döndürür.\n",
    "words_map = words.map(lambda x: (x, 1))\n",
    "words_map.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "15\n",
      "scalajavahadoopsparkakkaspark vs hadooppysparkpyspark and spark\n",
      "scala-java-hadoop-spark-akka-spark vs hadoop-pyspark-pyspark and spark\n",
      "words_concat: <class 'str'>\n",
      "1-2-3-4-5\n"
     ]
    }
   ],
   "source": [
    "# reduce(): Kendisine parametre olarak verilen fonksiyonu bütün elemanlar üzerinde gerçekleştirir. Map fonksiyonundan farkı sanırım şu; map eleman birleştirme vb. gibi işlemler yapmıyor. Bunun adında anlaşılacağı üzere reduce azaltıyor, küçültüyor.\n",
    "from operator import add\n",
    "nums = sc.parallelize([1, 2, 3, 4, 5])\n",
    "print(nums.reduce(add))\n",
    "print(words.reduce(add))\n",
    "\n",
    "def my_add(a, b):\n",
    "    return f'{a}-{b}'\n",
    "\n",
    "words_concat = words.reduce(my_add)\n",
    "print(words_concat)\n",
    "print(\"words_concat:\", type(words_concat))\n",
    "print(nums.reduce(my_add))\n",
    "\n",
    "# 2' den fazla parametre alan fonksiyonları çalıştırmıyor sanırım.\n",
    "#def my_add_2(a, b, c):\n",
    "#    return f'{a}-{b}-{c}'\n",
    "\n",
    "# print(words.reduce(my_add_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('hadoop', (4, 5)), ('spark', (1, 2)), ('spark', (1, 16)), ('spark', (1, 100))]\n",
      "[('hadoop', (5, 4)), ('spark', (2, 1)), ('spark', (16, 1)), ('spark', (100, 1))]\n"
     ]
    }
   ],
   "source": [
    "# join(): İki farklı RDD' nin key' lerini baz alarak value değerlerini birleştirir. RDD_A içindeki eleman (k, v1), RDD_2 içindeki eleman (k, v2) ise onları (k, (v1, v2)) şekilinde birleştiriyor. Peki böyle bir şey neden var? Peki neden (k, v) şeklinde olmak zorunda?\n",
    "x = sc.parallelize([(\"spark\", 1), (\"hadoop\", 4), (\"dene\", 4)])\n",
    "y = sc.parallelize([(\"spark\", 2), (\"hadoop\", 5), ('spark', 16, 17), (\"naber\", 4), ['spark', 100]])\n",
    "# , {'spark': 200} koydum y' ye hata verdi.\n",
    "joined = x.join(y)\n",
    "print(joined.collect())\n",
    "\n",
    "joined = y.join(x)\n",
    "print(joined.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True\nFalse\nTrue\nFalse\n"
     ]
    }
   ],
   "source": [
    "# cache(): Persist(kalıcı) this RDD with the default storage level (MEMORY_ONLY).\n",
    "# işlemlerin çok daha hızlı olması içindir. Belki Spark işlemi gerçekleştirdikten sonra RAM' den siliyor. Bu fonksiyon sayesinde verileri RAM' de kalıcı yapıyor\n",
    "\n",
    "print(words.persist().is_cached)\n",
    "# persist(): fonksiyonu' da kalıcı yapıyor. Fonksiyonun açıklamasını oku.\n",
    "# is_cahched RDD nesnesine ait bir değişken\n",
    "\n",
    "print(nums.is_cached)\n",
    "print(nums.persist().is_cached)\n",
    "print(nums.unpersist().is_cached)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark",
   "language": "python",
   "name": "spark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}