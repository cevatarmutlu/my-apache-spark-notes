{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [RDD Programming Guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Overview](https://spark.apache.org/docs/latest/rdd-programming-guide.html#overview)\n",
    "\n",
    "**High level** her Spark uygulaması ***driver program*** denen bir şeyden oluşur. Bu ***driver program*** kullanıcının *main* fonksiyonunu çalıştırır ve **cluster** üzerinde çeşitli paralel işlemleri execute eder.\n",
    "\n",
    "Spark' ın sağladığı **<font color=\"red\">main absraction</font>** ***resilient distributed dataset*** (RDD) olmaktadır. RDD, bir elemanlar kümesidir(collection of elements). RDD, cluster üzerindeki node' lar boyunca partition' lara (bölümlere) ayrılmıştır. RDD' lerin node' lara bölünmesi sayesinde paralel işlemler yapılabilmektedir.\n",
    "\n",
    "RDD' ler çeşitli formattaki dosyalardan ya da driver program içindeki collectionlardan oluşturulabilir (collectiondan kasıt sanırım scala dizileri ya da hangi dili kullanıyorsanız).\n",
    "\n",
    "Eğer bir RDD' yi çok sık şekilde kullanacaksanız Spark' a bu RDD' yi cache al diyebiliyorsunuz.\n",
    "\n",
    "RDD' ler node' larda oluşan hatalardan otomatik olarak recover edilirler Sanırım hiç hata olmamış hallerine geri döndürülürler.\n",
    "\n",
    "___\n",
    "\n",
    "Spark' ın **<font color=\"red\">ikinci absraction</font>**' u paralel işlemler kullanılan ***shared variables*** tir.\n",
    "\n",
    "Sanırım Spark' ta Default olarak şöyle bir şey gerçekleştiriliyor: Spark' ta paralel bir işlem başladığında node' lara task' lar dağıtılıyor. Bu task' lar için gerekli olan variable' lar node' larda çalışacak olan fonksiyona kopyalanıyor.\n",
    "\n",
    "Bazen bir tane değişkenin tasklar arasında ya da task ile driver program arasında shared edilmesi gerekiyormuş. Bu özelliğin var olma sebebi bu.\n",
    "\n",
    "Sparkta iki tane shared variables türü varmış:\n",
    "***broadcast variables***: Tüm nodeların memory' lerindeki bir value' yi cache almak için kullanılır. (Ne demek anlamadım.)\n",
    "***accumulators***: Yanlızca \"added\" değişkenlermiş. Örneğin counters ve sum. (Ne demek anlamadım)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Resilient Distributed Datasets (RDDs)](https://spark.apache.org/docs/latest/rdd-programming-guide.html#resilient-distributed-datasets-rdds)\n",
    "\n",
    "Spark ***Resilient Distributed Datasets*** kavramı etrafında döner. RDD, paralel olarak çalıştırılabilen, hata toleranslı elementler koleksiyonudur (**fault-tolerant** collection of elements).\n",
    "\n",
    "RDD iki şekilde oluşturulur: <br/>\n",
    "birincisi driver programda bulunan colloection' u ***parallelizing*** ederek. <br/>\n",
    "ikincisi external storage system' da bulunan bir dataset' e referans vererek. external storage system, such as a shared filesystem, HDFS, HBase, or any data source offering a Hadoop InputFormat.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Parallelized Collections](https://spark.apache.org/docs/latest/rdd-programming-guide.html#parallelized-collections)\n",
    "\n",
    "Bir parallelized collection, sizin driver programınızda bulunan collection' ı parametre olarak olan ***SparkContext***' in ***parallelize*** metodu ile oluşturulur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: Array[Int] = Array(1, 2, 3, 4, 5)\n",
       "distData: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:26\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = Array(1, 2, 3, 4, 5)\n",
    "val distData = sc.parallelize(data) //RDD oluştu ve paralel işlem yapılabilir artık.\n",
    "// Normally, Spark tries to set the number of partitions automatically based on your cluster. \n",
    "// However, you can also set it manually by passing it as a second parameter to parallelize "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parallel collection' larda önemli bir konuda bu RDD' nin kaç tane partition' a bölüneceği. Spark normalde partion' lama işlemlerini kendi yaparmış. Ama elle biz de verebilirmişiz. Elle vermek istersek `sc.parallelize(data, 10)` bu şekilde yapabiliriz.\n",
    "\n",
    "> Bazı yerlerde `partions` yerine `slices` ifadesi geçebilirmiş."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [External Datasets](https://spark.apache.org/docs/latest/rdd-programming-guide.html#external-datasets)\n",
    "\n",
    "Spark can create distributed datasets from any storage source supported by Hadoop, including your local file system, HDFS, Cassandra, HBase, Amazon S3, etc. Spark supports text files, SequenceFiles, and any other Hadoop InputFormat.\n",
    "\n",
    "Text file RDDs can be created using ***SparkContext***’s ***textFile*** method. This method takes a URI for the file (either a local path on the machine, or a hdfs://, s3a://, etc URI) and reads it as a collection of lines. Here is an example invocation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.rdd.RDD\n",
       "distFİle: org.apache.spark.rdd.RDD[String] = ../README.md MapPartitionsRDD[1] at textFile at <console>:27\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.rdd.RDD;\n",
    "\n",
    "val distFile: RDD[String] = sc.textFile(\"../README.md\") //distFile' in türünü atamasını Scala' nın bulması yerine\n",
    "// kendim atadığım için compile edilirken daha hızlı çalışacaktır."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some notes on reading files with Spark:\n",
    "- Bir tane ana bilgisayar var ve ona bağlı köle bilgisayarlar (worker node) var. Ana bilgisayarın local' ındeki file' a köle bilgisayarlar tabiki de erişemeyecek. Erişebilmesi için ya dosyayayı workerlara kopyala ya da internet üzerinden erişmelerini sağla\n",
    "- All of Spark’s file-based input methods, including textFile, support running on directories, compressed files, and wildcards as well. For example, you can use textFile(\"/my/directory\"), textFile(\"/my/directory/*.txt\"), and textFile(\"/my/directory/*.gz\").\n",
    "- `textFile` metoduna isteğe bağlı RDD' nin partition' ını belirlemek için ikinci bir parametre daha verebiliyorsun. Default olarak bu partition' lara bölme işlemini `Spark zaten dosyadaki her bir block için bir tane partition oluşturarak yapıyor (HDFS' te her bir block 128MB' tır)`. İşte textFile metodunun ikinci parametresi sayesinde bu sayıyı arttırabiliyoruz. block değerinden aşağı bir partition sayısı belirleyemiyoruz. Default block size: 128MB. block size 256MB yapılamaz. `Default partition değeri 5 ise bu değeri parametre ile 4 yapamazsın. Çünkü partiton size' ı 128MB' den büyük olur.`\n",
    "\n",
    "> Devamıda vardı yazmadım. İlgili olan başlığa tıklayarak gerekli linke gidebilir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [RDD Operations](https://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-operations)\n",
    "\n",
    "RDD' ler iki tür işleme destek verirler:\n",
    "- **transformation**: var olan bir RDD' den yeni bir RDD elde etme.\n",
    "- **actions**: Bir RDD' den belli hesaplamalar ile driver program' a bir değer döndürülmesi işlemleri.\n",
    "\n",
    "Transformation işlemleri ile oluşan RDD' ler hemen oluşturulmazlarmış(transformation işlemleri saklanırmış). Eğer bir action gelirse RDD oluşturulur ve daha sonra action işlemi gerçekleştirilirmiş. Bu Spark' a verimlilik katarmış (Ben ikna olmadım).\n",
    "\n",
    "Default olarak her action işlemi ile RDD oluşur ve üzerinden işlem yapılırmış ama istersek oluşan RDD' yi ***persist*** hale getirebiliyormuşuz: `action` ya da `persist` metodları ile. Eğer RDD üzerinde çok işlem yapılacaksa persist hale getirmek verimliliği arttır. (Buna ikna oldum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Basic](https://spark.apache.org/docs/latest/rdd-programming-guide.html#basics)\n",
    "> Spark' ın doc' u hakkikaten güzel anlatmış.\n",
    "\n",
    "To illustrate RDD basics, consider the simple program below:\n",
    "```scala\n",
    "val lines = sc.textFile(\"data.txt\")\n",
    "val lineLengths = lines.map(s => s.length)\n",
    "val totalLength = lineLengths.reduce((a, b) => a + b)\n",
    "```\n",
    "The first line defines a base RDD from an external file. This dataset is not loaded in memory or otherwise acted on: lines is merely a pointer to the file. The second line defines lineLengths as the result of a map transformation. Again, lineLengths is not immediately computed, due to laziness. Finally, we run reduce, which is an action. At this point Spark breaks the computation into tasks to run on separate machines, and each machine runs both its part of the map and a local reduction, returning only its answer to the driver program.\n",
    "\n",
    "If we also wanted to use lineLengths again later, we could add:\n",
    "\n",
    "```scala\n",
    "lineLengths.persist()\n",
    "```\n",
    "before the reduce, which would cause lineLengths to be saved in memory after the first time it is computed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Basic](https://spark.apache.org/docs/latest/rdd-programming-guide.html#basics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
