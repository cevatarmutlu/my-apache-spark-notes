{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Interactive Analysis with the Spark Shell](https://spark.apache.org/docs/latest/quick-start.html#interactive-analysis-with-the-spark-shell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ENG: Spark’s primary abstraction is a distributed collection of items called a Dataset. Datasets can be created from Hadoop InputFormats (such as HDFS files) or by transforming other Datasets.\n",
    "\n",
    "TR: Spark' ın `birincil soyutlaması(primary abstraction)` `item'ların dağıtık bir koleksiyonu(distributed collection of items)` olan Datasetlerdir. Datasetler Hadoop InputFormats (HDFS dosyalarıyla) ya da diğer datasetlerden dönüştürülerek oluşturulabilir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://172.18.0.1:4040\n",
       "SparkContext available as 'sc' (version = 3.0.1, master = local[*], app id = local-1613816717137)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "textFile: org.apache.spark.sql.Dataset[String] = [value: string]\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val textFile = spark.read.textFile(\"../README.md\") //Projede bulunan README.md \n",
    "//text dosyasından yeni bir Dataset oluşturuyoruz.\n",
    "\n",
    "// Sanırım:\n",
    "// Bir Datasetten veri alma işlemine action deniyor.\n",
    "// Bir Datasetin itemlarını kullanarak yeni bir Dataset oluşturmaya transform deniyor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: Long = 8\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Bir Dataset üzerinde yapılabilecek bazı action' lar\n",
    "textFile.count() // Number of items in this Dataset. TR: README.md dosyasında bulunan satır sayısı.\n",
    "//Demekki item diye bahsettiği şey aslında satır sayısıymış."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res5: String = ## Giriş\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Bir Dataset üzerinde yapılabilecek bazı action' lar\n",
    "textFile.first() // First item in this Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lineWithSpark: org.apache.spark.sql.Dataset[String] = [value: string]\r\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Transform işlemi (bir datasetten yeni bir dataset oluşturma) için filter adlı bir fonksiyon kullandık. \n",
    "// İçinde Spark geçen item' lardan yeni bir tane Dataset oluşturduk.\n",
    "val lineWithSpark = textFile.filter(line => line.contains(\"Spark\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res8: Long = 2\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Transform ve action işlemlerini bu şekilde birleştirebiliyoruz.\n",
    "textFile.filter(line => line.contains(\"Spark\")).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More on Dataset Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res11: Int = 22\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Daha karmaşık hesaplamalar için transform ve action işlemlerini kullanabiliriz.\n",
    "// Mesela en çok kelime bulunan satırı bulmak isteyelim.\n",
    "textFile.map(line => line.split(\" \").size).reduce((a, b) => if (a > b) a else b)\n",
    "// İlk olarak satırları integer sayılara map edip yeni bir Dataset oluşturuyoruz. reduce fonksiyonu yeni oluşan\n",
    "// Dataset tarafından çağrılır ve en yüksek kelime sayısını bulur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import java.lang.Math\n",
       "res13: Int = 22\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// map ve reduce fonksiyonlarının argümanları Scala funtion literals (closures) özelliğindedir. [Python'daki iterator]\n",
    "// Bu yüzden bu fonksiyonlar herhangi bir language feature ya da Scala/ Java kütüphanesinden \n",
    "// belirli feature' i alabilir. Mesela yukarıdaki işlemi daha okunabilir yapalım.\n",
    "import java.lang.Math\n",
    "\n",
    "textFile.map(line => line.split(\" \").size).reduce((a,b) => Math.max(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "textFile' daki item sayısı: 8\n",
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|            ## Giriş|\n",
      "|                    |\n",
      "|Apache Spark ile ...|\n",
      "|                    |\n",
      "|~~Apache Spark' ı...|\n",
      "|                    |\n",
      "|                    |\n",
      "|Scala kullanmaya ...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Tam olarak ne yaptığını  görmek istiyorum.\n",
    "println(\"textFile' daki item sayısı: \" + textFile.count())\n",
    "textFile.show() // README.md dosyasındaki satırları görüyorsunuz. README.md dosyasında toplamda 8 satır var.\n",
    "// Hiç bir şey yazmayan satırlarıda item olarak alıyor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tempDataset' teki item sayısı: 8\n",
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|         [##, Giriş]|\n",
      "|                  []|\n",
      "|[Apache, Spark, i...|\n",
      "|                  []|\n",
      "|[~~Apache, Spark'...|\n",
      "|                  []|\n",
      "|                  []|\n",
      "|[Scala, kullanmay...|\n",
      "+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tempDataset: org.apache.spark.sql.Dataset[Array[String]] = [value: array<string>]\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var tempDataset = textFile.map(line => line.split(\" \"))\n",
    "println(\"tempDataset' teki item sayısı: \" + tempDataset.count())\n",
    "tempDataset.show() //map ile transform, show ile action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|    2|\n",
      "|    1|\n",
      "|    6|\n",
      "|    1|\n",
      "|   22|\n",
      "|    1|\n",
      "|    1|\n",
      "|   10|\n",
      "+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "newTextFileDataset: org.apache.spark.sql.Dataset[Int] = [value: int]\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val newTextFileDataset = textFile.map(line => line.split(\" \").size) // map ile transform, show ile action\n",
    "newTextFileDataset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res27: Int = 22\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newTextFileDataset.reduce((a, b) => if (a > b) a else b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wordCounts: org.apache.spark.sql.Dataset[(String, Long)] = [key: string, count(1): bigint]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// One common data flow pattern is MapReduce, as popularized by Hadoop. Spark can implement MapReduce flows easily\n",
    "val wordCounts = textFile.flatMap(line => line.split(\" \")).groupByKey(identity).count()\n",
    "\n",
    "// flat map ile satır Dataset' i olan textFile' ı kelime datasetine çeviriyor. \n",
    "// groupByKey ve count ile kelimelerin sayısını elde ediliyor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: Array[(String, Long)] = Array((şekilde,1), (ı,1), (notlarımı,1), (Giriş,1), (Sistemi,1), (kullanmaya,1), (olarakta,1), (~~Apache,1), (yapabiliyormuşuz.,1), (verdim.,2), (Linux,1), (kullanmaktayım.,1), (dağıtım,1), (Apache,1), (olarak,1), (bu,1), (işlemleri,1), (Ubuntu,1), (Scala,1), (yazmaya,1), (İşletim,1), (yaptım.~~,1), (PySpark,1), (ile,3), (Python,1), (Spark',1), (##,1), (Spark,1), (ilgili,1), (Jupyter,1), (\"\",4), (tutacağım.,1), (karar,2), (kullandım.,1), ([şu](./kurulum.md),1), (notebook,1), (Kurulumu,1))\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordCounts.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "textFile item count: 8\n",
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|            ## Giriş|\n",
      "|                    |\n",
      "|Apache Spark ile ...|\n",
      "|                    |\n",
      "|~~Apache Spark' ı...|\n",
      "|                    |\n",
      "|                    |\n",
      "|Scala kullanmaya ...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "println(\"textFile item count: \" + textFile.count())\n",
    "textFile.show() // textFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wordDataset item count: 44\n",
      "+----------+\n",
      "|     value|\n",
      "+----------+\n",
      "|        ##|\n",
      "|     Giriş|\n",
      "|          |\n",
      "|    Apache|\n",
      "|     Spark|\n",
      "|       ile|\n",
      "|    ilgili|\n",
      "| notlarımı|\n",
      "|tutacağım.|\n",
      "|          |\n",
      "|  ~~Apache|\n",
      "|    Spark'|\n",
      "|         ı|\n",
      "|    Python|\n",
      "|       ile|\n",
      "|   yazmaya|\n",
      "|     karar|\n",
      "|   verdim.|\n",
      "|   PySpark|\n",
      "|kullandım.|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "wordDataset: org.apache.spark.sql.Dataset[String] = [value: string]\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Şimdi yukarıdaki işlemleri teker teker yapalım\n",
    "val wordDataset = textFile.flatMap(line => line.split(\" \"))\n",
    "println(\"wordDataset item count: \" + wordDataset.count())\n",
    "wordDataset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------+\n",
      "|              key|count(1)|\n",
      "+-----------------+--------+\n",
      "|          şekilde|       1|\n",
      "|                ı|       1|\n",
      "|        notlarımı|       1|\n",
      "|            Giriş|       1|\n",
      "|          Sistemi|       1|\n",
      "|       kullanmaya|       1|\n",
      "|         olarakta|       1|\n",
      "|         ~~Apache|       1|\n",
      "|yapabiliyormuşuz.|       1|\n",
      "|          verdim.|       2|\n",
      "|            Linux|       1|\n",
      "|  kullanmaktayım.|       1|\n",
      "|          dağıtım|       1|\n",
      "|           Apache|       1|\n",
      "|           olarak|       1|\n",
      "|               bu|       1|\n",
      "|        işlemleri|       1|\n",
      "|           Ubuntu|       1|\n",
      "|            Scala|       1|\n",
      "|          yazmaya|       1|\n",
      "+-----------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "workDatasetKeyCountPair: org.apache.spark.sql.Dataset[(String, Long)] = [key: string, count(1): bigint]\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val workDatasetKeyCountPair = wordDataset.groupByKey(identity).count() // Burada tam olarak nasıl key count\n",
    "// yapıyor anlamadım. Çıktı falan da göremediğim için nasıl yapıyor diye bir mantık kuramadım.\n",
    "workDatasetKeyCountPair.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res17: Array[(String, Long)] = Array((şekilde,1), (ı,1), (notlarımı,1), (Giriş,1), (Sistemi,1), (kullanmaya,1), (olarakta,1), (~~Apache,1), (yapabiliyormuşuz.,1), (verdim.,2), (Linux,1), (kullanmaktayım.,1), (dağıtım,1), (Apache,1), (olarak,1), (bu,1), (işlemleri,1), (Ubuntu,1), (Scala,1), (yazmaya,1), (İşletim,1), (yaptım.~~,1), (PySpark,1), (ile,3), (Python,1), (Spark',1), (##,1), (Spark,1), (ilgili,1), (Jupyter,1), (\"\",4), (tutacağım.,1), (karar,2), (kullandım.,1), ([şu](./kurulum.md),1), (notebook,1), (Kurulumu,1))\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workDatasetKeyCountPair.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching\n",
    "\n",
    "Bu kısmı tam olarak anlamadım.\n",
    "\n",
    "Spark bildiğim kadarıyla verileri RAM' a alarak işliyor. Hadoop' tan farkı buradan ortaya çıkıyor. Hadoop verileri Disk üzerinden işler Spark ise RAM üzerinden veriler üzerinden işlem yapar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res3: lineWithSpark.type = [value: string]\r\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lineWithSpark.cache() // Şuan veriler cache' e alındı"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res4: Long = 2\r\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lineWithSpark.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Self-Contained Applications diye bir kısım vardı. Ona bakmadım. Scala projesinde Spark kullanmayı gösteriyor sanırım"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
