{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Interactive Analysis with the Spark Shell](https://spark.apache.org/docs/latest/quick-start.html#interactive-analysis-with-the-spark-shell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Basic](https://spark.apache.org/docs/latest/quick-start.html#basics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ENG: Spark’s primary abstraction is a distributed collection of items called a Dataset. Datasets can be created from Hadoop InputFormats (such as HDFS files) or by transforming other Datasets.\n",
    "\n",
    "TR: Spark' ın `birincil soyutlaması(primary abstraction)` `Dataset`lerdir. `Dataset` dediği şey ise okuduğu dosyadaki `item` olarak kabul ettiği şeylerin kümesidir. `Dataset` HDSF dosyalarından ya da farklı kaynaklardan da oluşturulabilir. `distributed collection of items` bu ifade de ki `distributed` ifadesi ne olabilir bilmiyorum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "textFile: org.apache.spark.sql.Dataset[String] = [value: string]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val textFile = spark.read.textFile(\"../README.md\") //Scala dizininde bulunan README.md dosyasını okuyor.\n",
    "// Text dosyasından yeni bir Dataset oluşturuyor.\n",
    "\n",
    "// Bir Dataset' ten value alma işlemine action deniyor.\n",
    "// Bir Dataset' ten yeni bir Dataset oluşturmaya da transform deniyor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res4: Long = 21\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Aşağıdaki bir action işlemidir.\n",
    "textFile.count() // Number of items in this Dataset.\n",
    "// README.md satırları bu örnekte item' dır"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res6: String = ## Açıklama\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Action\n",
    "textFile.first() // First item in this Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lineWithSpark: org.apache.spark.sql.Dataset[String] = [value: string]\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Transform: Bir Dataset' ten yeni bir Dataset oluşturma işlemine denir.\n",
    "// Elimizdeki Dataset'ten yeni bir Dataset oluşturmak için filter isimli bir fonksiyon çağıracağız.\n",
    "// filter fonksiyonu ile elimizdeki Dataset' in item' larının alt kümesinden yeni bir Dataset oluşturacağız.\n",
    "val lineWithSpark = textFile.filter(line => line.contains(\"Spark\")) \n",
    "// line => line.contains(\"Spark\") -> Bu kısım Python' daki lambda fonksiyonu ile aynıdır.\n",
    "// line isimli parametre alan bir fonksiyon var ve bu fonksiyon içinde \"Spark\" geçen item' lardan yeni bir Dataset\n",
    "// oluşturuyor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines contains Spark: 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "res11: Long = 3\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Action\n",
    "println(\"Number of lines contains Spark: \" + lineWithSpark.count())\n",
    "\n",
    "// Transform ve action işlemlerini bu şekilde birleştirebiliyoruz.\n",
    "textFile.filter(line => line.contains(\"Spark\")).count()\n",
    "\n",
    "// Jupyter Notebook' ta hücrenin en sonundaki satır out kısmında yazılır yani print edilmesine gerek yoktur.\n",
    "// Üst satırlardaki işlemler ise print edilmedilir. Bu sebeple ilk satırda print varken ikincisinde yoktur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [More on Dataset Operations](https://spark.apache.org/docs/latest/quick-start.html#more-on-dataset-operations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Action ve Transform işlemlerini kullanarak daha karmaşık hesaplamalar yapılabilir. Mesela elimizdeki Dataset' teki en çok kelime barındıran satırın kaç kelime barındırdığını bulmak istersek:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: Int = 12\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile.map(line => line.split(\" \").size).reduce((a, b) => if (a > b) a else b)\n",
    "// Bu işlemi teker teker açıklamak istiyorum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[value: int]\n",
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|    2|\n",
      "|    1|\n",
      "|    6|\n",
      "|    1|\n",
      "|    5|\n",
      "+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "textFileLineWordCount: org.apache.spark.sql.Dataset[Int] = [value: int]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// map(line => line.split(\" \").size)\n",
    "\n",
    "/*\n",
    "    map: Kendisine parametre olarak verilen fonksiyonu Dataset' teki bütün item' lara uygular\n",
    "    ve Dataset' ten yeni bir Dataset oluşturur. Yeni oluşturduğu Dataset' in item değerleri\n",
    "    olarak kendisine parametre olarak verilen fonksiyonun çıktısını yazar.\n",
    "\n",
    "    Map fonksiyonuna parametre olarak verilen fonksiyon sayıları 2 ile çarpsın. `1, 2, 3` değerlerini\n",
    "    item olarak tutan Dataset' in map fonksiyonu çıktısı: `2, 4, 6` olur (1 * 2, 2 * 2, 3 * 2).\n",
    "*/\n",
    "\n",
    "/*\n",
    "    Anonymous Functions: Python' daki lambda function' un Scala karşılığıdır. Bilmeyenler için\n",
    "    kısa ve isimsiz bir şekilde fonksiyon tanımlama yoludur.\n",
    "\n",
    "    val sum = (num1: Int, num2: Int) => num1 + num2\n",
    "\n",
    "    Okun sol tarafı fonksiyonun parametreleridir. Sağ tarafı ise return değeridir.\n",
    "\n",
    "    sum(5, 6) => çıktı 11' dir.\n",
    "*/\n",
    "\n",
    "/*\n",
    "    line => line.split(\" \").size\n",
    "\n",
    "    Yukarıdaki Anonymous Function kendisine parametre olarak verilen String ifadeyi split fonksiyonu ile\n",
    "    parçalayıp split fonksiyonunun döndürdüğü Array' in size' ını yani boyutunu return eder.\n",
    "\n",
    "*/\n",
    "\n",
    "val textFileLineWordCount = textFile.map(line => line.split(\" \").size)\n",
    "\n",
    "println(textFileLineWordCount) // textFileLineWordCount Dataset' indeki item' ların type' ları int' tir. Out kısmında bakın.\n",
    "\n",
    "textFileLineWordCount.show(numRows=5)\n",
    "// textFile Dataset' inde item değerleri String' tir(textFile' ı okuduğumuz cell' e bakın).\n",
    "// Bu String tipi Spark' a özel bir String tipi değildir.\n",
    "// Scala' nın String type' ıdır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: Int = 12\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// reduce((a, b) => if (a > b) a else b)\n",
    "\n",
    "/*\n",
    "    reduce: map gibi parametre olarak fonksiyon alır. Parametre olarak aldığı fonksiyonun iki parametresi ve tek return' ü\n",
    "    olmalıdır.\n",
    "\n",
    "    reduce kelime anlamı olarak azaltmak demek. Dataset' teki item' ları ikişer olarak ele alıp belirli işlemleri\n",
    "    uygulamaya yarar. Max, Min vb. işlemlerdir. reduce işlemini tam olarak nasıl yapar bilmiyorum.\n",
    "\n",
    "*/\n",
    "\n",
    "/*\n",
    "    (a, b) => if (a > b) a else b\n",
    "\n",
    "    Yukarıdaki ifade bir Anonymous Function'dır ve kendisine parametre olarak verilen iki değerden en büyüğünü\n",
    "    return eder.\n",
    "*/\n",
    "\n",
    "// reduce((a, b) => if (a > b) a else b)\n",
    "// Yukarıdaki işlem item' ları ikişer ikişer karşılaştırarak en büyük değer olanı elde etmeye yarar.\n",
    "\n",
    "textFile.map(line => line.split(\" \").size).reduce((a, b) => if (a > b) a else b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import java.lang.Math\n",
       "res13: Int = 22\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.lang.Math\n",
    "\n",
    "textFile.map(line => line.split(\" \").size).reduce((a,b) => Math.max(a, b))\n",
    "// Okunurluğu ya da verimliliği arttırmak için Scala dilinin destekleklediği her şey kullanılabilir.\n",
    "// Math.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wordCounts: org.apache.spark.sql.Dataset[(String, Long)] = [key: string, count(1): bigint]\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// One common data flow pattern is MapReduce, as popularized by Hadoop. Spark can implement MapReduce flows easily\n",
    "val wordCounts = textFile.flatMap(line => line.split(\" \")).groupByKey(identity).count()\n",
    "\n",
    "// Yukarıdaki işlemi teker teker anlatmak istiyorum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|      [##, Açıklama]|\n",
      "|                  []|\n",
      "|[ScalaSpark, ile,...|\n",
      "|                  []|\n",
      "|[###, Klasör, isi...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+\n",
      "|     value|\n",
      "+----------+\n",
      "|        ##|\n",
      "|  Açıklama|\n",
      "|          |\n",
      "|ScalaSpark|\n",
      "|       ile|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "flatTextFile: org.apache.spark.sql.Dataset[String] = [value: string]\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// flatMap(line => line.split(\" \"))\n",
    "\n",
    "/*\n",
    "    flatMap: map fonksiyonu ile farkı şu: Dataset' in kaç item' ı varsa map fonksiyonunun çıktısı olan Dataset' de\n",
    "    o kadar item' a sahiptir. flatMap' te ise ya aynı sayıda item' a ya da daha fazla sayıda item' a sahip olur.\n",
    "\n",
    "    Eğer flatMap' e parametre olarak verilen fonksiyonun çıktısı tek bir değer dönüyorsa map gibi çalışır fakat\n",
    "    birden fazla değer dönüyorsa dönülen her değer yeni Dataset' te item olur.\n",
    "    \n",
    "*/\n",
    "\n",
    "\n",
    "textFile.map(line => line.split(\" \")).show(numRows=5) // map\n",
    "val flatTextFile = textFile.flatMap(line => line.split(\" \")) //flatMap\n",
    "flatTextFile.show(numRows=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "// groupByKey(identity)\n",
    "\n",
    "/*\n",
    "    identity: identity bir Scala fonksiyonudur. Kendine parametre olarak verilen değeri return eder.\n",
    "    \n",
    "    identity(5) => 5 döner.\n",
    "*/\n",
    "\n",
    "// groupByKey ifadesini örnek ile anlatacağım."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|class|number|\n",
      "+-----+------+\n",
      "|    A|    -5|\n",
      "|    A|    -5|\n",
      "|    A|    -5|\n",
      "|    B|    -4|\n",
      "|    C|    -3|\n",
      "+-----+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---+--------+\n",
      "|key|count(1)|\n",
      "+---+--------+\n",
      "| 5A|       3|\n",
      "| 4B|       1|\n",
      "| 3C|       2|\n",
      "| 2D|       1|\n",
      "| 1F|       1|\n",
      "+---+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "columns: Seq[String] = List(class, number)\n",
       "data: Seq[(String, Int)] = List((A,-5), (A,-5), (A,-5), (B,-4), (C,-3), (C,-3), (D,-2), (F,-1), (H,0), (H,0), (H,0), (H,0), (H,0), (K,1), (K,1), (K,1), (K,1), (L,2), (L,2), (M,3), (S,4), (T,5))\n",
       "df: org.apache.spark.sql.DataFrame = [class: string, number: int]\n",
       "groupedDf: org.apache.spark.sql.Dataset[(String, Long)] = [key: string, count(1): bigint]\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// İlk önce bir Dataset oluşturalım. Aşağıdaki gibi bir Dataset oluşturduğumuzda item' ın tipi\n",
    "// Dataset[Row] olur. Yukarıdaki örnekte ise Dataset[String]' ti. Bu bilgi önemli.\n",
    "\n",
    "// Data oluşturma\n",
    "val columns = Seq(\"class\" ,\"number\")\n",
    "val data = Seq(\n",
    "    (\"A\", -5), (\"A\", -5), (\"A\", -5), (\"B\", -4), (\"C\", -3), (\"C\", -3), (\"D\", -2), (\"F\", -1), \n",
    "    (\"H\", 0), (\"H\", 0), (\"H\", 0), (\"H\", 0), (\"H\", 0), (\"K\", 1), (\"K\", 1), (\"K\", 1), (\"K\", 1), \n",
    "    (\"L\", 2), (\"L\", 2), (\"M\", 3), (\"S\", 4), (\"T\", 5)\n",
    ")\n",
    "\n",
    "val df = spark.sparkContext.parallelize(data).toDF(columns:_*) // _* ne demek bilmiyorum.\n",
    "df.show(numRows=5)\n",
    "// Cell' in Out kısmına bakarsanız df' in DataFrame olduğunu görürsünüz. DataFrame demek Dataset[Row] demektir.\n",
    "\n",
    "val groupedDf = df.groupByKey(row => (row.getInt(1)).abs + row.getString(0)).count()\n",
    "/*\n",
    "    row => (row.getInt(1)).abs + row.getString(0)\n",
    "\n",
    "    (\"A\", -5) item' ımız olsun. Bu işlemde yapılan her şey Dataset' teki bütün item' lara uygulanır.\n",
    "\n",
    "    row.getInt(1).abs => {\n",
    "        row.getInt(1) bu değer -5 değer\n",
    "        abs ile -5 değerinin mutlak değeri alınır yani 5 olur\n",
    "    }\n",
    "    NOT: getInt olmasının sebebi Dataset' teki item' lar Row nesnesi cinsinden tutulduğu içindir. 1 değerinin\n",
    "    anlamı ise Int değerinin Row' un `1.` kolonunda olmasından.\n",
    "\n",
    "    row.getString(0) => {\n",
    "        A değeri çekilmiş olur.\n",
    "    }\n",
    "\n",
    "    Sonuç olarak 5A değeri elde edilir ve bu değer yeni Dataset' in item değeri olarak yazılır. Kolon adı ise\n",
    "    key olur. Cell' deki ikinci çıktıya bakınız.\n",
    "\n",
    "    Yukarıdaki işlem bütün item' lara uygulanır ve elde edilen Dataset' üzerinde key kolonu için groupBy işlemi uygulanır.\n",
    "*/\n",
    "\n",
    "/*\n",
    "    count: count bir aggregation işlemidir ve gruplanan ifadelerin kaç tane olduğunu bize verir.\n",
    "*/\n",
    "\n",
    "groupedDf.show(numRows=5) // Görmek için ikinci çıktıya bakınız."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wordCounts: org.apache.spark.sql.Dataset[(String, Long)] = [key: string, count(1): bigint]\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val wordCounts = textFile.flatMap(line => line.split(\" \")).groupByKey(identity).count()\n",
    "// Yukarıdaki işlem groupBy ile de yapabilir lakin böyle bir şey tercih edilmiş.\n",
    "\n",
    "/*\n",
    "Yukarıdaki işlemde ise flatMap fonksiyonun kendisi tek kolonlu bir Dataset döndürür: Dataset[String]. Bu Dataset' e DatasetA diyeceğim.\n",
    "\n",
    "Yukarıdaki işlem => {\n",
    "    DatasetA' daki bütün item' lara teker teker identity fonksiyonu uygulanır ve yeni bir Dataset elde edilir: DatasetB\n",
    "    DatasetB' de key kolonuna göre gruplanıp count alınır.\n",
    "}\n",
    "*/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res7: Array[(String, Long)] = Array((Okuduğum,1), ([Jupyter,1), ([Read,1), (dökümanlar,1), ([install](scala/install),1), (code,1), (işlemler:,1), (*,7), (Interactive,1), (--------,1), (with,1), (barındıran,1), (RDD,1), (Analysis,1), (Kurulumlar,1), (Programming,1), (yazma:,1), (açıklama,1), (klasör.,1), (notebook',1), (okuma,,1), (doc,1), (Shell](scala/doc/scala_doc_01.ipynb),1), (dosyaları,1), (|,5), ([SparkDoc:,2), (------,1), (Linkler,1), (Başka,1), (iligli,1), (the,1), (ScalaSpark,1), (ve,1), (Belirli,1), (kaynak](https://medium.com/@bogdan.cojocar/how-to-run-scala-and-spark-in-the-jupyter-notebook-328a80090b3b),1), ([code](scala/code),1), (yazma,1), (and,1), (Scala,1), (barındırdıkları,1), (JSON,1), ([doc](scala/doc),1), (ile,1), (###,1), (install,1), (##,2), (klasör,1), (ta,1), (S...\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordCounts.collect() // Dataset' i bir Array' a dönüştüren fonksiyon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Caching](https://spark.apache.org/docs/latest/quick-start.html#caching)\n",
    "\n",
    "Bu kısmı tam olarak anlamadım. Araştırıp detaylı olarak yazmayı düşünüyorum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Self-Contained Applications diye bir kısım vardı. Ona bakmadım."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
