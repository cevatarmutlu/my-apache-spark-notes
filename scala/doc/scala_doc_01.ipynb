{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Interactive Analysis with the Spark Shell](https://spark.apache.org/docs/latest/quick-start.html#interactive-analysis-with-the-spark-shell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Basic](https://spark.apache.org/docs/latest/quick-start.html#basics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ENG: Spark’s primary abstraction is a distributed collection of items called a Dataset. Datasets can be created from Hadoop InputFormats (such as HDFS files) or by transforming other Datasets.\n",
    "\n",
    "TR: Spark' ın `birincil soyutlaması(primary abstraction)` `Dataset`lerdir. `Dataset` dediği şey ise okuduğu dosyadaki `item` olarak kabul ettiği şeylerin kümesidir. `Dataset` HDSF dosyalarından ya da farklı kaynaklardan da oluşturulabilir. `distributed collection of items` bu ifade de ki `distributed` ifadesi ne olabilir bilmiyorum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "textFile: org.apache.spark.sql.Dataset[String] = [value: string]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val textFile = spark.read.textFile(\"../README.md\") //Scala dizininde bulunan README.md dosyasını okuyor.\n",
    "// Text dosyasından yeni bir Dataset oluşturuyor.\n",
    "\n",
    "// Bir Dataset' ten value alma işlemine action deniyor.\n",
    "// Bir Dataset' ten yeni bir Dataset oluşturmaya da transform deniyor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res4: Long = 21\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Aşağıdaki bir action işlemidir.\n",
    "textFile.count() // Number of items in this Dataset.\n",
    "// README.md satırları bu örnekte item' dır"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res6: String = ## Açıklama\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Action\n",
    "textFile.first() // First item in this Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lineWithSpark: org.apache.spark.sql.Dataset[String] = [value: string]\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Transform: Bir Dataset' ten yeni bir Dataset oluşturma işlemine denir.\n",
    "// Elimizdeki Dataset'ten yeni bir Dataset oluşturmak için filter isimli bir fonksiyon çağıracağız.\n",
    "// filter fonksiyonu ile elimizdeki Dataset' in item' larının alt kümesinden yeni bir Dataset oluşturacağız.\n",
    "val lineWithSpark = textFile.filter(line => line.contains(\"Spark\")) \n",
    "// line => line.contains(\"Spark\") -> Bu kısım Python' daki lambda fonksiyonu ile aynıdır.\n",
    "// line isimli parametre alan bir fonksiyon var ve bu fonksiyon içinde \"Spark\" geçen item' lardan yeni bir Dataset\n",
    "// oluşturuyor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines contains Spark: 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "res11: Long = 3\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Action\n",
    "println(\"Number of lines contains Spark: \" + lineWithSpark.count())\n",
    "\n",
    "// Transform ve action işlemlerini bu şekilde birleştirebiliyoruz.\n",
    "textFile.filter(line => line.contains(\"Spark\")).count()\n",
    "\n",
    "// Jupyter Notebook' ta hücrenin en sonundaki satır out kısmında yazılır yani print edilmesine gerek yoktur.\n",
    "// Üst satırlardaki işlemler ise print edilmedilir. Bu sebeple ilk satırda print varken ikincisinde yoktur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [More on Dataset Operations](https://spark.apache.org/docs/latest/quick-start.html#more-on-dataset-operations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Action ve Transform işlemlerini kullanarak daha karmaşık hesaplamalar yapılabilir. Mesela elimizdeki Dataset' teki en çok kelime barındıran satırın kaç kelime barındırdığını bulmak istersek:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: Int = 12\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile.map(line => line.split(\" \").size).reduce((a, b) => if (a > b) a else b)\n",
    "// Bu işlemi teker teker açıklamak istiyorum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[value: int]\n",
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|    2|\n",
      "|    1|\n",
      "|    6|\n",
      "|    1|\n",
      "|    5|\n",
      "+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "textFileLineWordCount: org.apache.spark.sql.Dataset[Int] = [value: int]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// map(line => line.split(\" \").size)\n",
    "\n",
    "/*\n",
    "    map: Kendisine parametre olarak verilen fonksiyonu Dataset' teki bütün item' lara uygular\n",
    "    ve parametre olarak verilen fonksiyonun çıktısını item' in yerine yazar.\n",
    "\n",
    "    Map fonksiyonuna parametre olarak verilen fonksiyon sayıları 2 ile çarpsın. 1, 2, 3 değerlerini\n",
    "    item olarak tutan Dataset' in map fonksiyonu çıktısı: 2, 4, 6 olur (1 * 2, 2 * 2, 3 * 2).\n",
    "*/\n",
    "\n",
    "/*\n",
    "    Anonymous Functions: Python' daki lambda function' un Scala karşılığıdır. Bilmeyenler için\n",
    "    kısa ve isimsiz bir şekilde fonksiyon tanımlama yoludur.\n",
    "\n",
    "    val sum = (num1: Int, num2: Int) => num1 + num2\n",
    "\n",
    "    Okun sol tarafı fonksiyonun parametreleridir. Sağ tarafı ise return değeridir.\n",
    "\n",
    "    sum(5, 6) => çıktı 11' dir.\n",
    "*/\n",
    "\n",
    "/*\n",
    "    line => line.split(\" \").size\n",
    "\n",
    "    Yukarıdaki Anonymous Function kendisine parametre olarak verilen String ifadeyi split fonksiyonu ile\n",
    "    parçalayıp split fonksiyonunun döndürdüğü Array' in size' ını yani boyutunu return eder.\n",
    "\n",
    "*/\n",
    "\n",
    "val textFileLineWordCount = textFile.map(line => line.split(\" \").size)\n",
    "\n",
    "println(textFileLineWordCount) // textFileLineWordCount Dataset' indeki item' ların type' ları int' tir.\n",
    "\n",
    "textFileLineWordCount.show(numRows=5)\n",
    "// textFile Dataset' inde item değerleri String' tir(textFile' ı okuduğumuz cell' e bakın).\n",
    "// Bu String tipi Spark' a özel bir String tipi değildir.\n",
    "// Scala' nın String type' ıdır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: Int = 12\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// reduce((a, b) => if (a > b) a else b)\n",
    "\n",
    "/*\n",
    "    reduce: map gibi parametre olarak fonksiyon alır. Parametre olarak aldığı fonksiyonun iki parametresi ve tek return' ü\n",
    "    olmalıdır.\n",
    "\n",
    "    reduce kelime anlamı olarak azaltmak demek. Dataset' teki item' ları ikişer olarak ele alıp belirli işlemleri\n",
    "    uygulamaya yarar. Max, Min vb. işlemlerdir.\n",
    "\n",
    "*/\n",
    "\n",
    "/*\n",
    "    (a, b) => if (a > b) a else b\n",
    "\n",
    "    Yukarıdaki ifade bir Anonymous Function'dır ve kendisine parametre olarak verilen iki değerden en büyüğünü\n",
    "    return eder.\n",
    "*/\n",
    "\n",
    "// reduce((a, b) => if (a > b) a else b)\n",
    "// Yukarıdaki işlem item' ları ikişer ikişer karşılaştırarak en büyük değer olanı elde etmeye yarar.\n",
    "// Tam olarak çalışma mantığı nasıldır bilmiyorum.\n",
    "\n",
    "textFile.map(line => line.split(\" \").size).reduce((a, b) => if (a > b) a else b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import java.lang.Math\n",
       "res13: Int = 22\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.lang.Math\n",
    "\n",
    "textFile.map(line => line.split(\" \").size).reduce((a,b) => Math.max(a, b))\n",
    "// Okunurluğu ya da verimliliği arttırmak için Scala dilinin destekleklediği her şey kullanılabilir.\n",
    "// Math.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wordCounts: org.apache.spark.sql.Dataset[(String, Long)] = [key: string, count(1): bigint]\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// One common data flow pattern is MapReduce, as popularized by Hadoop. Spark can implement MapReduce flows easily\n",
    "val wordCounts = textFile.flatMap(line => line.split(\" \")).groupByKey(identity).count()\n",
    "\n",
    "// Yukarıdaki işlemi teker teker anlatmak istiyorum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|      [##, Açıklama]|\n",
      "|                  []|\n",
      "|[ScalaSpark, ile,...|\n",
      "|                  []|\n",
      "|[###, Klasör, isi...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+\n",
      "|     value|\n",
      "+----------+\n",
      "|        ##|\n",
      "|  Açıklama|\n",
      "|          |\n",
      "|ScalaSpark|\n",
      "|       ile|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// flatMap(line => line.split(\" \"))\n",
    "\n",
    "/*\n",
    "    flatMap: map fonksiyonu ile farklı şu. Dataset' in kaç item' ı varsa map fonksiyonunun çıktısı olan Dataset' de\n",
    "    o kadar item' a sahiptir. flatMap' te ise ya aynı sayıda item' a ya da daha fazla sayıda item' a sahip olur.\n",
    "\n",
    "    Eğer flatMap' e parametre olarak verilen fonksiyonun çıktısı tek bir değer dönüyorsa map gibi çalışır fakat\n",
    "    birden fazla değer dönüyorsa dönülen her değer yeni Dataset' te item olur.\n",
    "    \n",
    "*/\n",
    "\n",
    "\n",
    "textFile.map(line => line.split(\" \")).show(numRows=5)\n",
    "textFile.flatMap(line => line.split(\" \")).show(numRows=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "28: error: overloaded method value groupByKey with alternatives:",
     "output_type": "error",
     "traceback": [
      "<console>:28: error: overloaded method value groupByKey with alternatives:",
      "  [K](func: org.apache.spark.api.java.function.MapFunction[(String, Int),K], encoder: org.apache.spark.sql.Encoder[K])org.apache.spark.sql.KeyValueGroupedDataset[K,(String, Int)] <and>",
      "  [K](func: ((String, Int)) => K)(implicit evidence$3: org.apache.spark.sql.Encoder[K])org.apache.spark.sql.KeyValueGroupedDataset[K,(String, Int)]",
      " cannot be applied to ()",
      "       textFile.flatMap(line => line.split(\" \")).map(word => (word, 1)).groupByKey().count()",
      "                                                                        ^",
      ""
     ]
    }
   ],
   "source": [
    "// groupByKey(identity)\n",
    "\n",
    "/*\n",
    "    identity: identity bir Scala fonksiyonudur. Kendine parametre olarak verilen değeri return eder.\n",
    "    \n",
    "    identity(5) => 5 döner.\n",
    "*/\n",
    "\n",
    "/*\n",
    "    groupByKey: Bu fonksiyon (K,V) pair' larını grouplamaya yarar.\n",
    "\n",
    "*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: Array[(String, Long)] = Array((şekilde,1), (ı,1), (notlarımı,1), (Giriş,1), (Sistemi,1), (kullanmaya,1), (olarakta,1), (~~Apache,1), (yapabiliyormuşuz.,1), (verdim.,2), (Linux,1), (kullanmaktayım.,1), (dağıtım,1), (Apache,1), (olarak,1), (bu,1), (işlemleri,1), (Ubuntu,1), (Scala,1), (yazmaya,1), (İşletim,1), (yaptım.~~,1), (PySpark,1), (ile,3), (Python,1), (Spark',1), (##,1), (Spark,1), (ilgili,1), (Jupyter,1), (\"\",4), (tutacağım.,1), (karar,2), (kullandım.,1), ([şu](./kurulum.md),1), (notebook,1), (Kurulumu,1))\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordCounts.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "textFile item count: 8\n",
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|            ## Giriş|\n",
      "|                    |\n",
      "|Apache Spark ile ...|\n",
      "|                    |\n",
      "|~~Apache Spark' ı...|\n",
      "|                    |\n",
      "|                    |\n",
      "|Scala kullanmaya ...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "println(\"textFile item count: \" + textFile.count())\n",
    "textFile.show() // textFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wordDataset item count: 44\n",
      "+----------+\n",
      "|     value|\n",
      "+----------+\n",
      "|        ##|\n",
      "|     Giriş|\n",
      "|          |\n",
      "|    Apache|\n",
      "|     Spark|\n",
      "|       ile|\n",
      "|    ilgili|\n",
      "| notlarımı|\n",
      "|tutacağım.|\n",
      "|          |\n",
      "|  ~~Apache|\n",
      "|    Spark'|\n",
      "|         ı|\n",
      "|    Python|\n",
      "|       ile|\n",
      "|   yazmaya|\n",
      "|     karar|\n",
      "|   verdim.|\n",
      "|   PySpark|\n",
      "|kullandım.|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "wordDataset: org.apache.spark.sql.Dataset[String] = [value: string]\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Şimdi yukarıdaki işlemleri teker teker yapalım\n",
    "val wordDataset = textFile.flatMap(line => line.split(\" \"))\n",
    "println(\"wordDataset item count: \" + wordDataset.count())\n",
    "wordDataset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------+\n",
      "|              key|count(1)|\n",
      "+-----------------+--------+\n",
      "|          şekilde|       1|\n",
      "|                ı|       1|\n",
      "|        notlarımı|       1|\n",
      "|            Giriş|       1|\n",
      "|          Sistemi|       1|\n",
      "|       kullanmaya|       1|\n",
      "|         olarakta|       1|\n",
      "|         ~~Apache|       1|\n",
      "|yapabiliyormuşuz.|       1|\n",
      "|          verdim.|       2|\n",
      "|            Linux|       1|\n",
      "|  kullanmaktayım.|       1|\n",
      "|          dağıtım|       1|\n",
      "|           Apache|       1|\n",
      "|           olarak|       1|\n",
      "|               bu|       1|\n",
      "|        işlemleri|       1|\n",
      "|           Ubuntu|       1|\n",
      "|            Scala|       1|\n",
      "|          yazmaya|       1|\n",
      "+-----------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "workDatasetKeyCountPair: org.apache.spark.sql.Dataset[(String, Long)] = [key: string, count(1): bigint]\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val workDatasetKeyCountPair = wordDataset.groupByKey(identity).count() // Burada tam olarak nasıl key count\n",
    "// yapıyor anlamadım. Çıktı falan da göremediğim için nasıl yapıyor diye bir mantık kuramadım.\n",
    "workDatasetKeyCountPair.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res17: Array[(String, Long)] = Array((şekilde,1), (ı,1), (notlarımı,1), (Giriş,1), (Sistemi,1), (kullanmaya,1), (olarakta,1), (~~Apache,1), (yapabiliyormuşuz.,1), (verdim.,2), (Linux,1), (kullanmaktayım.,1), (dağıtım,1), (Apache,1), (olarak,1), (bu,1), (işlemleri,1), (Ubuntu,1), (Scala,1), (yazmaya,1), (İşletim,1), (yaptım.~~,1), (PySpark,1), (ile,3), (Python,1), (Spark',1), (##,1), (Spark,1), (ilgili,1), (Jupyter,1), (\"\",4), (tutacağım.,1), (karar,2), (kullandım.,1), ([şu](./kurulum.md),1), (notebook,1), (Kurulumu,1))\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workDatasetKeyCountPair.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching\n",
    "\n",
    "Bu kısmı tam olarak anlamadım.\n",
    "\n",
    "Spark bildiğim kadarıyla verileri RAM' a alarak işliyor. Hadoop' tan farkı buradan ortaya çıkıyor. Hadoop verileri Disk üzerinden işler Spark ise RAM üzerinden veriler üzerinden işlem yapar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res3: lineWithSpark.type = [value: string]\r\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lineWithSpark.cache() // Şuan veriler cache' e alındı"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res4: Long = 2\r\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lineWithSpark.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Self-Contained Applications diye bir kısım vardı. Ona bakmadım. Scala projesinde Spark kullanmayı gösteriyor sanırım"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
