{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.1.152:4040\n",
       "SparkContext available as 'sc' (version = 3.2.0, master = local[*], app id = local-1643562649392)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@5964129f\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Spark Session başlatılır\n",
    "\n",
    "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
    "\n",
    "val spark: SparkSession = SparkSession\n",
    "      .builder()\n",
    "      .appName(\"Read and Write operations with CSV files\")\n",
    "      .master(\"local\")\n",
    "      .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "base_path: String = /home/cevat/workspace/my-apache-spark-notes/scala/projects/read_write_csv/src/csv\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val base_path: String = \"/home/cevat/workspace/my-apache-spark-notes/scala/projects/read_write_csv/src/csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---+---------+-------+\n",
      "|    _c0|       _c1|_c2|      _c3|    _c4|\n",
      "+-------+----------+---+---------+-------+\n",
      "|   name|   surname|age|     city|country|\n",
      "|  Cevat|   Armutlu| 26| Istanbul| Turkey|\n",
      "|   John|      Eric| 56| New York|America|\n",
      "|Natalia|Khrushchev|  6|Moskovitz|Russiaa|\n",
      "+-------+----------+---+---------+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "single_csv_path: String = /home/cevat/workspace/my-apache-spark-notes/scala/projects/read_write_csv/src/csv/test1.csv\n",
       "first_csv_df: org.apache.spark.sql.DataFrame = [_c0: string, _c1: string ... 3 more fields]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Read Single CSV\n",
    "val single_csv_path: String = base_path + \"/test1.csv\"\n",
    "val first_csv_df: DataFrame = spark.read.csv(path = single_csv_path)\n",
    "first_csv_df.show()\n",
    "\n",
    "// CSV dosyası okunda ve bir DataFrame nesnesine dönüştürüldü."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                 _c0|\n",
      "+--------------------+\n",
      "|del1;del2;del3;de...|\n",
      "|delcol1;delcol1;d...|\n",
      "|delcol1;delcol1;d...|\n",
      "|                name|\n",
      "|               Cevat|\n",
      "|                John|\n",
      "|             Natalia|\n",
      "|               col11|\n",
      "|               test1|\n",
      "|               test1|\n",
      "|               test1|\n",
      "|                col1|\n",
      "|                  a1|\n",
      "|                  b1|\n",
      "|                  c1|\n",
      "+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "csv_folder: String = /home/cevat/workspace/my-apache-spark-notes/scala/projects/read_write_csv/src/csv\n",
       "read_csv_folder: org.apache.spark.sql.DataFrame = [_c0: string]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Read Multiple CSV: Folder\n",
    "val csv_folder: String = base_path\n",
    "val read_csv_folder: DataFrame = spark.read.csv(path = csv_folder)\n",
    "// Herhangi bir okuma sırası var mı anlamadım. test4 dosyasını eklediğimde ilk onu okuyor. Sildiğimde test1.csv' yi okuyor.\n",
    "// test4 base_path dizininde.\n",
    "read_csv_folder.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                 _c0|\n",
      "+--------------------+\n",
      "|del1;del2;del3;de...|\n",
      "|delcol1;delcol1;d...|\n",
      "|delcol1;delcol1;d...|\n",
      "|                name|\n",
      "|               Cevat|\n",
      "|                John|\n",
      "|             Natalia|\n",
      "|                col1|\n",
      "|                  a1|\n",
      "|                  b1|\n",
      "|                  c1|\n",
      "+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "read_multi_files: org.apache.spark.sql.DataFrame = [_c0: string]\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Read Multiple CSV: Multiple paths\n",
    "val read_multi_files = spark.read\n",
    "    .csv(\n",
    "        base_path + \"/test2.csv\",\n",
    "        base_path + \"/test4.csv\",\n",
    "        base_path + \"/test1.csv\"\n",
    "    ) // String* demek birden fazla String ifade girebilirsin demek\n",
    "// Folder' ın içindeki CSV dosyalarını okutmak gibi. \n",
    "// CSV dosyalarının yazılış sırasının bir önemi yok.\n",
    "\n",
    "\n",
    "/*\n",
    "    NOT: csv(\"file1.csv,file2.csv,file3.csv\")\n",
    "*/\n",
    "\n",
    "read_multi_files.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---+---------+-------+\n",
      "|   name|   surname|age|     city|country|\n",
      "+-------+----------+---+---------+-------+\n",
      "|  Cevat|   Armutlu| 26| Istanbul| Turkey|\n",
      "|   John|      Eric| 56| New York|America|\n",
      "|Natalia|Khrushchev|  6|Moskovitz|Russiaa|\n",
      "+-------+----------+---+---------+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "read_csv_with_header: org.apache.spark.sql.DataFrame = [name: string, surname: string ... 3 more fields]\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Read CSV file with header: Single CSV file\n",
    "// Bizim okuduğumuz CSV dosyalarında Header bulunmaktaydı yani kolonların isimleri.\n",
    "// İlk okumamızda kolon isimleri olarak _c0, _c1 yazmasının sebebi Spark' a CSV dosyasındaki\n",
    "// ilk satırın header olduğunu belirtmediğimiz için.\n",
    "\n",
    "\n",
    "val read_csv_with_header: DataFrame = spark\n",
    "      .read\n",
    "      .option(\"header\", \"true\") // Bu option ifadesi ile Spark' a ilk satırın header olduğunu belirtmiş oluyoruz.\n",
    "      .csv(single_csv_path)\n",
    "\n",
    "// header option değeri Default olarka false' tur.\n",
    "// Header option' ı Read/Write işlemlerinin ikisinde de kullanılabilir.\n",
    "read_csv_with_header.show()\n",
    "\n",
    "// Daha fazla option için: https://spark.apache.org/docs/latest/sql-data-sources-csv.html#data-source-option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|del1;del2;del3;del4;del5;del6;del7|\n",
      "+----------------------------------+\n",
      "|              delcol1;delcol1;d...|\n",
      "|              delcol1;delcol1;d...|\n",
      "|                             Cevat|\n",
      "|                              John|\n",
      "|                           Natalia|\n",
      "|                             test1|\n",
      "|                             test1|\n",
      "|                             test1|\n",
      "|                                a1|\n",
      "|                                b1|\n",
      "|                                c1|\n",
      "+----------------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "read_csv_folder_with_header: org.apache.spark.sql.DataFrame = [del1;del2;del3;del4;del5;del6;del7: string]\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Read CSV file with header: Multiple CSV file\n",
    "val read_csv_folder_with_header: DataFrame = spark\n",
    "      .read\n",
    "      .option(\"header\", \"true\")\n",
    "      .csv(path = csv_folder)\n",
    "// Böyle bir durumda ilk okuduğu dosyanın ilk satırı header olarak almaktadır.\n",
    "\n",
    "read_csv_folder_with_header.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+-------+-------+-------+-------+\n",
      "|   del1|   del2|   del3|   del4|   del5|   del6|   del7|\n",
      "+-------+-------+-------+-------+-------+-------+-------+\n",
      "|delcol1|delcol1|delcol1|delcol1|delcol1|delcol1|delcol1|\n",
      "|delcol2|delcol2|delcol2|delcol2|delcol2|delcol2|delcol2|\n",
      "+-------+-------+-------+-------+-------+-------+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "read_csv_file_with_delimiter: org.apache.spark.sql.DataFrame = [del1: string, del2: string ... 5 more fields]\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Read CSV file with delimiter\n",
    "// CSV dosyalarında kolonlar arasındaki değerler `,` ile ayrılır. Lakin bazı dosyalarıda başka\n",
    "// karakterlerde kullanılabilir.\n",
    "\n",
    "val read_csv_file_with_delimiter: DataFrame = spark\n",
    "    .read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"delimiter\", \";\") // Bu option ile kolon değerlerinin hangi karakter ile ayrıldığı belirtiliyor.\n",
    "    .csv(base_path + \"/test4.csv\") //test4 dosyasında kolonlar `;` ile ayrılıyor.\n",
    "\n",
    "// Default değer: `,`\n",
    "// Birden fazla karakter değeri de verilebilir.\n",
    "\n",
    "read_csv_file_with_delimiter.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+-------+-------+-------+-------+\n",
      "|   del1|   del2|   del3|   del4|   del5|   del6|   del7|\n",
      "+-------+-------+-------+-------+-------+-------+-------+\n",
      "|delcol1|delcol1|delcol1|delcol1|delcol1|delcol1|delcol1|\n",
      "|delcol2|delcol2|delcol2|delcol2|delcol2|delcol2|delcol2|\n",
      "+-------+-------+-------+-------+-------+-------+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "read_csv_file_with_delimiter: org.apache.spark.sql.DataFrame = [del1: string, del2: string ... 5 more fields]\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Read CSV file with sep\n",
    "// sep ifadesi delimiter ile aynıdır.\n",
    "\n",
    "val read_csv_file_with_delimiter: DataFrame = spark\n",
    "    .read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"sep\", \";\")\n",
    "    .csv(base_path + \"/test4.csv\") //test4 dosyasında kolonlar `;` ile ayrılıyor.\n",
    "read_csv_file_with_delimiter.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+-----+\n",
      "|col1|col2|col3| col4|\n",
      "+----+----+----+-----+\n",
      "|  a1|  a2|  a3|a4,a5|\n",
      "|  b1|  b2|  b3|   b4|\n",
      "|  c1|  c2|  c3|   c4|\n",
      "+----+----+----+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "read_csv_file_with_quote: org.apache.spark.sql.DataFrame = [col1: string, col2: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Read CSV file with quote\n",
    "// Bazen CSV dosyalarındaki değerler delimiter karakterini içerebilir.\n",
    "// Bu gibi durumlarda ise CSV dosyasında bu değer belirli bir karakterin içinde yazılır: \"a1, a5\" gibi.\n",
    "// İşte bu gibi değerlerin hangi karakterin içine yazıldığını bu option ile belirliyoruz.\n",
    "\n",
    "val read_csv_file_with_quote: DataFrame = spark\n",
    "      .read\n",
    "      .option(\"header\", \"true\")\n",
    "      .option(\"delimiter\", \",\")\n",
    "      .option(\"quote\", \"'\") // Kolon değeri içinde , kullanmak için değeri 'a1, a5' şeklinde yazdığını belirtiyorsun.\n",
    "      .csv(base_path + \"/test2.csv\")\n",
    "\n",
    "// Default değeri: Çift tırnaktır.\n",
    "    \n",
    "read_csv_file_with_quote.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- testcol1: integer (nullable = true)\n",
      " |-- testcol2: integer (nullable = true)\n",
      " |-- testcol3: double (nullable = true)\n",
      "\n",
      "+--------+--------+--------+\n",
      "|testcol1|testcol2|testcol3|\n",
      "+--------+--------+--------+\n",
      "|     123|      -4|    12.4|\n",
      "|      32|      -5|    45.6|\n",
      "|      43|      -6|    67.8|\n",
      "+--------+--------+--------+\n",
      "\n",
      "root\n",
      " |-- testcol1: string (nullable = true)\n",
      " |-- testcol2: string (nullable = true)\n",
      " |-- testcol3: string (nullable = true)\n",
      "\n",
      "+--------+--------+--------+\n",
      "|testcol1|testcol2|testcol3|\n",
      "+--------+--------+--------+\n",
      "|     123|      -4|    12.4|\n",
      "|      32|      -5|    45.6|\n",
      "|      43|      -6|    67.8|\n",
      "+--------+--------+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "read_csv_file_with_inferSchema: org.apache.spark.sql.DataFrame = [testcol1: int, testcol2: int ... 1 more field]\n",
       "read_csv_file_without_inferSchema: org.apache.spark.sql.DataFrame = [testcol1: string, testcol2: string ... 1 more field]\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Read CSV file with inferSchema\n",
    "\n",
    "// Bu option ile CSV dosyasındaki kolonların tipi Spark belirliyor.\n",
    "val read_csv_file_with_inferSchema: DataFrame = spark\n",
    "      .read\n",
    "      .option(\"header\", \"true\")\n",
    "      .option(\"delimiter\", \",\")\n",
    "      .option(\"quote\", \"'\")\n",
    "      .option(\"inferSchema\", \"true\")\n",
    "      .csv(base_path + \"/test5.csv\")\n",
    "      \n",
    "read_csv_file_with_inferSchema.printSchema()\n",
    "read_csv_file_with_inferSchema.show()\n",
    "\n",
    "// without inferSchema\n",
    "val read_csv_file_without_inferSchema: DataFrame = spark\n",
    "      .read\n",
    "      .option(\"header\", \"true\")\n",
    "      .option(\"delimiter\", \",\")\n",
    "      .option(\"quote\", \"'\")\n",
    "      .csv(base_path + \"/test5.csv\")\n",
    "      \n",
    "read_csv_file_without_inferSchema.printSchema()\n",
    "read_csv_file_without_inferSchema.show()\n",
    "\n",
    "// Bütün kolonları String olarak okudu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-------------------+\n",
      "| name|surname|               date|\n",
      "+-----+-------+-------------------+\n",
      "|cevat|armutlu|1995-09-12 00:00:00|\n",
      "|ahmet| mehmet|1888-04-28 00:00:00|\n",
      "+-----+-------+-------------------+\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- surname: string (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "read_csv_file_with_timestampFormat: org.apache.spark.sql.DataFrame = [name: string, surname: string ... 1 more field]\n"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Read CSv file with timestampFormat\n",
    "// dateFormat ile çalıştıramadım.\n",
    "\n",
    "val read_csv_file_with_timestampFormat: DataFrame = spark\n",
    "  .read\n",
    "  .option(\"header\", \"true\")\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .option(\"timestampFormat\", \"dd-MM-yyyy\") // timestamp formatını belirttiğimiz option\n",
    "  .csv(base_path + \"/test6.csv\")\n",
    "\n",
    "// timestamp format değerleri için: https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n",
    "\n",
    "read_csv_file_with_timestampFormat.show()\n",
    "read_csv_file_with_timestampFormat.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-------------------+\n",
      "| name|surname|               date|\n",
      "+-----+-------+-------------------+\n",
      "|cevat|armutlu|1995-09-12 00:00:00|\n",
      "|ahmet| mehmet|1888-04-28 00:00:00|\n",
      "+-----+-------+-------------------+\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- surname: string (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "read_csv_file_with_timestampFormat: org.apache.spark.sql.DataFrame = [name: string, surname: string ... 1 more field]\n"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Read CSV file with Schema: 1\n",
    "\n",
    "val read_csv_file_with_schema: DataFrame = spark\n",
    "    .read\n",
    "    .schema(\"name string, surname string, date date\") // Schema bu şekilde belirtilebilir. Kolon adı ve tipi şeklinde.\n",
    "    .options(Map(\n",
    "    \"header\" -> \"true\",\n",
    "    \"inferSchema\" -> \"true\",\n",
    "    \"delimiter\" -> \",\",\n",
    "    \"dateFormat\" -> \"dd-MM-yyyy\"\n",
    "    ))\n",
    "    .csv(base_path + \"/test6.csv\")\n",
    "\n",
    "read_csv_file_with_schema.printSchema()\n",
    "read_csv_file_with_schema.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- surname: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      "\n",
      "+-----+-------+----------+\n",
      "| name|surname|      date|\n",
      "+-----+-------+----------+\n",
      "| null|armutlu|1995-09-12|\n",
      "|ahmet| mehmet|1888-04-28|\n",
      "+-----+-------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types._\n",
       "struct: org.apache.spark.sql.types.StructType = StructType(StructField(name,StringType,false), StructField(surname,StringType,false), StructField(date,DateType,false))\n",
       "df: org.apache.spark.sql.DataFrame = [name: string, surname: string ... 1 more field]\n"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "\n",
    "val struct: StructType = StructType(Array(\n",
    "    StructField(name = \"name\", dataType = StringType, nullable = false),\n",
    "    StructField(name = \"surname\", dataType = StringType, nullable = false),\n",
    "    StructField(name = \"date\", dataType = DateType, nullable = false)\n",
    "))\n",
    "\n",
    "val df: DataFrame = spark\n",
    "    .read\n",
    "    .schema(struct)\n",
    "    .options(Map(\n",
    "    \"header\" -> \"true\", // true değil \"true\" olmalı\n",
    "    \"delimiter\" -> \",\",\n",
    "    \"dateFormat\" -> \"dd-MM-yyyy\",\n",
    "    \"nullValue\" -> \"cevat\" // Hangi değerin null olarak yazılacağı belirtilir. cevat isimli değerler yerine null yazar.\n",
    "    ))\n",
    "    .csv(base_path + \"/test6.csv\")\n",
    "\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
